{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;html&gt;\\n   &lt;HEAD&gt;\\n      &lt;meta http-equiv=\"Con...</td>\n",
       "      <td>Recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>Recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...</td>\n",
       "      <td>Recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt; &lt;...</td>\n",
       "      <td>Recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;!-- This page is cached by the Hummingbird Pe...</td>\n",
       "      <td>Recreation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content    category\n",
       "0  <html>\\n   <HEAD>\\n      <meta http-equiv=\"Con...  Recreation\n",
       "1  <?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!DOCT...  Recreation\n",
       "2  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...  Recreation\n",
       "3  <!DOCTYPE html>\\n<html lang=\"en-US\">\\n<head> <...  Recreation\n",
       "4  <!-- This page is cached by the Hummingbird Pe...  Recreation"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Establish connection using SQLAlchemy\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password@localhost:5432/dataset_bakalarka')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT content, category\n",
    "FROM (\n",
    "  SELECT content, category,\n",
    "         ROW_NUMBER() OVER (PARTITION BY category ORDER BY RANDOM()) AS rn\n",
    "  FROM web_data\n",
    "  WHERE category = 'Recreation'\n",
    ") sub\n",
    "WHERE rn <= 100000\n",
    "\"\"\"\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_sql_query(query, engine, chunksize=10000):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_19428\\2064381985.py:4: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(text, \"lxml\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "Recreation    35030\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    body = soup.body\n",
    "    return body.get_text(separator=\" \") if body else \"\"\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_html)\n",
    "df.head()\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "is_gpu_enabled = spacy.require_gpu()\n",
    "print(f\"Is GPU enabled: {is_gpu_enabled}\")\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Process the text through the spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    # Return the lemmatized text\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-English samples removed: 973\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply the language detection function\n",
    "df['is_english'] = df['clean_content'].apply(is_english)\n",
    "\n",
    "# Calculate the number of non-English samples\n",
    "non_english_count = df['is_english'].value_counts().get(False, 0)\n",
    "print(f\"Number of non-English samples removed: {non_english_count}\")\n",
    "\n",
    "# Filter out non-English samples\n",
    "df = df[df['is_english']].drop(columns=['is_english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;html&gt;\\n   &lt;HEAD&gt;\\n      &lt;meta http-equiv=\"Con...</td>\n",
       "      <td>Recreation</td>\n",
       "      <td>stud dog litters memories lexi malta gucci cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>Recreation</td>\n",
       "      <td>hawk vision color imagine art aspire merge enh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...</td>\n",
       "      <td>Recreation</td>\n",
       "      <td>book surf spain surf tours andalucia lanzarote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt; &lt;...</td>\n",
       "      <td>Recreation</td>\n",
       "      <td>continuation tradition wine wine wine vineyard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;!-- This page is cached by the Hummingbird Pe...</td>\n",
       "      <td>Recreation</td>\n",
       "      <td>mixtape aliquam lorem ante dapibus viverra qui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content    category  \\\n",
       "0  <html>\\n   <HEAD>\\n      <meta http-equiv=\"Con...  Recreation   \n",
       "1  <?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!DOCT...  Recreation   \n",
       "2  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...  Recreation   \n",
       "3  <!DOCTYPE html>\\n<html lang=\"en-US\">\\n<head> <...  Recreation   \n",
       "4  <!-- This page is cached by the Hummingbird Pe...  Recreation   \n",
       "\n",
       "                                       clean_content  \n",
       "0  stud dog litters memories lexi malta gucci cal...  \n",
       "1  hawk vision color imagine art aspire merge enh...  \n",
       "2  book surf spain surf tours andalucia lanzarote...  \n",
       "3  continuation tradition wine wine wine vineyard...  \n",
       "4  mixtape aliquam lorem ante dapibus viverra qui...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "with open(\"stopwords-en.txt\", \"r\") as file:\n",
    "    stopwords_list = file.read().splitlines()\n",
    "\n",
    "stopwords = set(stopwords_list)\n",
    "custom_stopwords = set([\n",
    "    'contact', 'service', 'policy', 'site', 'privacy', 'support', 'email', 'blog',\n",
    "    'post', 'learn', 'read', 'offer', 'provide', 'include', 'click', 'update',\n",
    "    'feature', 'link', 'search', 'website', 'program', 'start', 'view', 'resource',\n",
    "    'experience', 'list', 'free', 'info', 'shop', 'video', 'share', 'member',\n",
    "    'add', 'start', 'work', 'order', 'day', 'people', 'history', 'office',\n",
    "    'time', 'year', 'event', 'national', 'state', 'high', 'month', 'week', 'open',\n",
    "    'cookies', 'menu', 'cart', 'browser', 'select', 'choose', 'hope', 'enjoy', 'disabled',\n",
    "    'facebook', 'twitter', 'youtube', 'instagram', 'account', 'cookie', 'subscribe',\n",
    "    'newsletter', 'sign', 'message', 'comment', 'form', 'login', 'user', 'member',\n",
    "    'join', 'write', 'update', 'search', 'review',\n",
    "    'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n",
    "    'september', 'october', 'november', 'december', 'year', 'today', 'yesterday', 'tomorrow', 'datum', 'date',\n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec',\n",
    "    'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun',\n",
    "    'regional', 'albuquerque', 'chicago', 'minneapolis', 'philadelphia', 'phoenix', 'rhode', 'island', 'scottsdale', 'washington', 'wisconsin', 'michigan',\n",
    "    'bay', 'beach', 'dakota', 'florida', 'georgia', 'hampshire', 'harbor', 'iowa', 'maine',  'missouri', 'park', 'virginia', 'vista', 'wisconsin', 'massachusetts',\n",
    "    'minnesota',\n",
    "    'skip', 'content', 'main', 'term', 'condition', 'toggle', 'navigation', 'wordpress', 'social', 'medium', 'upcoming', 'event',\n",
    "    'photo', 'gallery', 'news', 'frequently', 'question', 'ask', 'press', 'release', 'quick', 'link', 'continue', 'read', 'phone', 'fax', 'answer', 'question',\n",
    "    'board', 'director', 'real', 'estate', 'los', 'angeles', 'new', 'york', 'city', 'san', 'francisco', 'power', 'united', 'kingdom', 'states', 'america', 'fran', 'ais',\n",
    "    'north', 'carolina', 'las', 'vegas', 'annual', 'report', 'highly', 'recommend', 'rss', 'feed', 'white', 'paper', 'hong', 'kong', 'credit', 'card', 'mental', 'health', 'public', 'save', 'money',\n",
    "    'annual', 'meeting', 'wide', 'range', 'care', 'gift', 'professional', 'live', 'stream', 'quality', 'product', 'project', 'management', 'meet', 'nonprofit', 'organization', 'blogthis', 'pinter',\n",
    "    'design', 'success', 'story', 'summer', 'camp', 'chain', 'register', 'trademark', 'username', 'password', 'certificate', 'plan', 'visit', 'regular', 'price', 'covid', 'pandemic', 'south', 'africa', 'west', 'east', 'regional',\n",
    "\n",
    "    'copyright', 'membership', 'links', 'online', 'check', 'sale', 'close', 'store', 'forum', 'local', 'create', 'location', 'american', 'company', 'community', 'late', 'hour',\n",
    "    'build', 'change', 'business', 'owner', 'class', 'set', 'leave', 'private', 'red', 'lot', 'posts', 'function', 'customers', 'deprecate', 'deprecated', 'httpdwww', 'getmagicquotesgpc',\n",
    "    'formattingphp', 'paragonskydivingcouk', 'events', 'follow', 'faq', 'association', 'picture', 'model', 'send', 'address', 'current', 'locate', 'radio', 'international', 'age', 'article',\n",
    "    'issue', 'base', 'texas', 'award', 'hold', 'result', 'bring', 'staff', 'receive', 'feel', 'county', 'require', 'type',\n",
    "    'mail', 'society', 'log', 'sell', 'donate', 'easy', 'note', 'access', 'schedule', 'purchase', 'center', 'black', 'blue', 'friendly', 'perfect',\n",
    "    'image', 'box', 'complete', 'happy', 'train', 'series', 'safe', 'watch', 'level', 'topic', 'item', 'resources', 'hand', 'party', 'light', 'land', 'personal', 'minute',\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "])\n",
    "stopwords.update(custom_stopwords)\n",
    "stopwords = sorted(stopwords)\n",
    "\n",
    "# Function to further clean the text\n",
    "def further_clean_text(text, stopwords):\n",
    "    # Normalize spaces; replaces all kinds of whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove all numbers (digits) from the text\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Convert text to lower case to standardize for stopwords removal\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split text into words, remove short words and stopwords\n",
    "    text = ' '.join([word for word in text.split() if len(word) >= 3 and word not in stopwords])\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(lambda x: further_clean_text(x, stopwords))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['club', 'wine', 'dog', 'book', 'family', 'car', 'breed', 'travel', 'calendar', 'reserve', 'trip', 'guide', 'fishing', 'life', 'fun', 'love', 'map', 'special', 'training', 'road', 'river', 'tour', 'fly', 'activity', 'hunt', 'team', 'puppy', 'water', 'volunteer', 'boat', 'country', 'lake', 'adventure', 'stay', 'rescue', 'collection', 'food', 'friend', 'beer', 'field', 'wines', 'cat', 'hunting', 'bird', 'winery', 'canada', 'safety', 'dive', 'camping', 'mountain', 'ride', 'sailing', 'explore', 'fish', 'valley', 'air', 'drive', 'guest', 'beautiful', 'house', 'school', 'pet', 'yacht', 'trail', 'night', 'diving', 'australia', 'vineyard', 'california', 'equipment', 'flight', 'season', 'weekend', 'cover', 'resort', 'charter', 'spring', 'weather', 'cruise', 'sport', 'win', 'holiday', 'outdoor', 'discover', 'tours', 'race', 'region', 'tasting', 'animal', 'game', 'bear', 'wildlife', 'walk', 'sea', 'lodge', 'scuba', 'islands', 'hotel', 'destination', 'vacation']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_content'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_keywords = [feature_names[i] for i in mean_tfidf.argsort()[::-1]]\n",
    "\n",
    "# Print as a Python array\n",
    "print(top_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyCaret)",
   "language": "python",
   "name": "pycaret_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
