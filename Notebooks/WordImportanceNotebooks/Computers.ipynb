{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\r\\n&lt;html xmlns=\"http://www.w3....</td>\n",
       "      <td>Computers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>Computers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt;&lt;l...</td>\n",
       "      <td>Computers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...</td>\n",
       "      <td>Computers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html  xmlns=\"http://www.w3.o...</td>\n",
       "      <td>Computers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content   category\n",
       "0  <!DOCTYPE html>\\r\\n<html xmlns=\"http://www.w3....  Computers\n",
       "1  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...  Computers\n",
       "2  <!doctype html>\\n<html lang=\"en-US\">\\n<head><l...  Computers\n",
       "3  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...  Computers\n",
       "4  <!DOCTYPE html>\\n<html  xmlns=\"http://www.w3.o...  Computers"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Establish connection using SQLAlchemy\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password@localhost:5432/dataset_bakalarka')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT content, category\n",
    "FROM (\n",
    "  SELECT content, category,\n",
    "         ROW_NUMBER() OVER (PARTITION BY category ORDER BY RANDOM()) AS rn\n",
    "  FROM web_data\n",
    "  WHERE category = 'Computers'\n",
    ") sub\n",
    "WHERE rn <= 100000\n",
    "\"\"\"\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_sql_query(query, engine, chunksize=10000):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_20828\\2064381985.py:4: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(text, \"lxml\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "Computers    26509\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    body = soup.body\n",
    "    return body.get_text(separator=\" \") if body else \"\"\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_html)\n",
    "df.head()\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "is_gpu_enabled = spacy.require_gpu()\n",
    "print(f\"Is GPU enabled: {is_gpu_enabled}\")\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Process the text through the spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    # Return the lemmatized text\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Apply lemmatization\n",
    "df['clean_content'] = df['clean_content'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-English samples removed: 465\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply the language detection function\n",
    "df['is_english'] = df['clean_content'].apply(is_english)\n",
    "\n",
    "# Calculate the number of non-English samples\n",
    "non_english_count = df['is_english'].value_counts().get(False, 0)\n",
    "print(f\"Number of non-English samples removed: {non_english_count}\")\n",
    "\n",
    "# Filter out non-English samples\n",
    "df = df[df['is_english']].drop(columns=['is_english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\r\\n&lt;html xmlns=\"http://www.w3....</td>\n",
       "      <td>Computers</td>\n",
       "      <td>close documentation tutorials templates alphab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>Computers</td>\n",
       "      <td>tallahasseepm loosely organize perl locate tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt;&lt;l...</td>\n",
       "      <td>Computers</td>\n",
       "      <td>blogs industries industries insurance companie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...</td>\n",
       "      <td>Computers</td>\n",
       "      <td>sushil consultants technology services company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html  xmlns=\"http://www.w3.o...</td>\n",
       "      <td>Computers</td>\n",
       "      <td>tel mail inforelavisioncom corporate profile r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content   category  \\\n",
       "0  <!DOCTYPE html>\\r\\n<html xmlns=\"http://www.w3....  Computers   \n",
       "1  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...  Computers   \n",
       "2  <!doctype html>\\n<html lang=\"en-US\">\\n<head><l...  Computers   \n",
       "3  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...  Computers   \n",
       "4  <!DOCTYPE html>\\n<html  xmlns=\"http://www.w3.o...  Computers   \n",
       "\n",
       "                                       clean_content  \n",
       "0  close documentation tutorials templates alphab...  \n",
       "1  tallahasseepm loosely organize perl locate tal...  \n",
       "2  blogs industries industries insurance companie...  \n",
       "3  sushil consultants technology services company...  \n",
       "4  tel mail inforelavisioncom corporate profile r...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "with open(\"stopwords-en.txt\", \"r\") as file:\n",
    "    stopwords_list = file.read().splitlines()\n",
    "\n",
    "stopwords = set(stopwords_list)\n",
    "custom_stopwords = set([\n",
    "    # First batch of words with no meaning\n",
    "    'contact', 'service', 'policy', 'site', 'privacy', 'support', 'email', 'blog',\n",
    "    'post', 'learn', 'read', 'offer', 'provide', 'include', 'click', 'update',\n",
    "    'feature', 'link', 'search', 'website', 'program', 'start', 'view', 'resource',\n",
    "    'experience', 'list', 'free', 'info', 'shop', 'video', 'share', 'member',\n",
    "    'add', 'start', 'work', 'order', 'day', 'people', 'history', 'office',\n",
    "    'time', 'year', 'event', 'national', 'state', 'high', 'month', 'week', 'open',\n",
    "    'cookies', 'menu', 'cart', 'browser', 'select', 'choose', 'hope', 'enjoy', 'disabled',\n",
    "    'facebook', 'twitter', 'youtube', 'instagram', 'account', 'cookie', 'subscribe',\n",
    "    'newsletter', 'sign', 'message', 'comment', 'form', 'login', 'user', 'member',\n",
    "    'join', 'write', 'update', 'search', 'review',\n",
    "    'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n",
    "    'september', 'october', 'november', 'december', 'year', 'today', 'yesterday', 'tomorrow', 'datum', 'date',\n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec',\n",
    "    'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun',\n",
    "    'regional', 'albuquerque', 'chicago', 'minneapolis', 'philadelphia', 'phoenix', 'rhode', 'island', 'scottsdale', 'washington', 'wisconsin', 'michigan',\n",
    "    'bay', 'beach', 'dakota', 'florida', 'georgia', 'hampshire', 'harbor', 'iowa', 'maine',  'missouri', 'park', 'virginia', 'vista', 'wisconsin', 'massachusetts',\n",
    "    'minnesota',\n",
    "    'skip', 'content', 'main', 'term', 'condition', 'toggle', 'navigation', 'wordpress', 'social', 'medium', 'upcoming', 'event',\n",
    "    'photo', 'gallery', 'news', 'frequently', 'question', 'ask', 'press', 'release', 'quick', 'link', 'continue', 'read', 'phone', 'fax', 'answer', 'question',\n",
    "    'board', 'director', 'real', 'estate', 'los', 'angeles', 'new', 'york', 'city', 'san', 'francisco', 'power', 'united', 'kingdom', 'states', 'america', 'fran', 'ais',\n",
    "    'north', 'carolina', 'las', 'vegas', 'annual', 'report', 'highly', 'recommend', 'rss', 'feed', 'white', 'paper', 'hong', 'kong', 'credit', 'card', 'mental', 'health', 'public', 'save', 'money',\n",
    "    'annual', 'meeting', 'wide', 'range', 'care', 'gift', 'professional', 'live', 'stream', 'quality', 'product', 'project', 'management', 'meet', 'nonprofit', 'organization', 'blogthis', 'pinter',\n",
    "    'design', 'success', 'story', 'summer', 'camp', 'chain', 'register', 'trademark', 'username', 'password', 'certificate', 'plan', 'visit', 'regular', 'price', 'covid', 'pandemic', 'south', 'africa', 'west', 'east', 'regional',\n",
    "\n",
    "\n",
    "    'business', 'customer', 'create', 'team', 'products', 'client', 'image', 'access', 'change', 'training', 'community', 'digital', 'control', 'language', 'platform', 'process', 'systems', 'industry', 'custom', 'set', 'follow',\n",
    "    'app', 'forum', 'late', 'send', 'demo', 'resources', 'library', 'fast', 'license', 'document', 'address', 'simple', 'google', 'manage', 'develop', 'request', 'performance', 'pro', 'device', 'store',\n",
    "    'marketing', 'type', 'partner', 'game', 'cost', 'center', 'drive', 'complete', 'require', 'format', 'result', 'enterprise', 'lead', 'manager', 'secure', 'improve', 'enable', 'issue', 'option', 'pdf', 'function', 'apple', 'core',\n",
    "    'local', 'recovery', 'memory', 'repair', 'jet', 'sale', 'life', 'media', 'personal', 'partners', 'level', 'market', 'job', 'note', 'global', 'word', 'step', 'single', 'series', 'hard', 'return',\n",
    "    'easy', 'deliver', 'book', 'overview', 'pricing', 'track', 'receive', 'focus', 'audio', 'increase', 'employee', 'integrate', 'easily', 'hour', 'compliance', 'title',\n",
    "    'trial', 'science', 'purchase', 'music', 'student', 'knowledge', 'expert', 'analysis', 'protect', 'lot', 'color', 'convert', 'object', 'events', 'english', 'engineering', 'future', 'learning', 'play', 'ensure', 'smart', 'speed', 'edit', 'item', 'minute',\n",
    "    'education', 'careers', 'love', 'bring', 'leave', 'basic', 'backup', 'tag', 'reserve', 'study', 'popular', 'pay', 'understand', 'button', 'map', 'explore', 'location', 'monitor', 'command', 'consulting', 'university', 'watch', 'risk', 'font',\n",
    "    'provider', 'win', 'record', 'automate', 'exist', 'financial', 'task', 'protection', 'archive', 'special', 'error'\n",
    "\n",
    "\n",
    "    \n",
    "])\n",
    "stopwords.update(custom_stopwords)\n",
    "stopwords = sorted(stopwords)\n",
    "\n",
    "# Function to further clean the text\n",
    "def further_clean_text(text, stopwords):\n",
    "    # Normalize spaces; replaces all kinds of whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove all numbers (digits) from the text\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Convert text to lower case to standardize for stopwords removal\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split text into words, remove short words and stopwords\n",
    "    text = ' '.join([word for word in text.split() if len(word) >= 3 and word not in stopwords])\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(lambda x: further_clean_text(x, stopwords))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['software', 'download', 'solution', 'file', 'services', 'company', 'online', 'technology', 'version', 'development', 'data', 'security', 'application', 'code', 'tool', 'copyright', 'solutions', 'windows', 'build', 'network', 'base', 'server', 'source', 'cloud', 'internet', 'mobile', 'check', 'linux', 'faq', 'database', 'mail', 'documentation', 'host', 'developer', 'technical', 'log', 'programming', 'guide', 'interface', 'standard', 'connect', 'close', 'mac', 'current', 'domain', 'hardware', 'theme', 'advanced', 'key', 'integration', 'virtual', 'automation', 'desktop', 'print', 'model', 'powerful', 'tools', 'class', 'article', 'java', 'package', 'machine', 'bug', 'display', 'api', 'engine', 'testing', 'tech', 'editor', 'environment', 'screen', 'javascript', 'suite', 'multiple', 'collection', 'field', 'size', 'communication', 'template', 'storage', 'plugin', 'multi', 'install', 'remote', 'module', 'ready', 'bit', 'directory', 'upgrade', 'tutorial', 'android', 'printer', 'script', 'implement', 'window', 'usb', 'excel', 'sql', 'xml', 'joomla']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_content'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_keywords = [feature_names[i] for i in mean_tfidf.argsort()[::-1]]\n",
    "\n",
    "# Print as a Python array\n",
    "print(top_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyCaret)",
   "language": "python",
   "name": "pycaret_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
