{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;HTML&gt;\\n&lt;HEAD&gt;\\n&lt;TITLE&gt;Gary Posner's Home Page...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt;\\n...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en-US\"&gt;&lt;head itemsc...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\r\\n\\r\\n&lt;!DOCTYPE html&gt;\\r\\n&lt;html lang=\"en\"&gt;\\r\\...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html xmlns=\"http://www.w3....</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content category\n",
       "0  <HTML>\\n<HEAD>\\n<TITLE>Gary Posner's Home Page...  Science\n",
       "1  <!doctype html>\\n<html lang=\"en-US\">\\n<head>\\n...  Science\n",
       "2  <!DOCTYPE html><html lang=\"en-US\"><head itemsc...  Science\n",
       "3  \\r\\n\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"en\">\\r\\...  Science\n",
       "4  <!DOCTYPE html>\\n\\n<html xmlns=\"http://www.w3....  Science"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Establish connection using SQLAlchemy\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password@localhost:5432/dataset_bakalarka')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT content, category\n",
    "FROM (\n",
    "  SELECT content, category,\n",
    "         ROW_NUMBER() OVER (PARTITION BY category ORDER BY RANDOM()) AS rn\n",
    "  FROM web_data\n",
    "  WHERE category = 'Science'\n",
    ") sub\n",
    "WHERE rn <= 100000\n",
    "\"\"\"\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_sql_query(query, engine, chunksize=10000):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_23680\\2064381985.py:4: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(text, \"lxml\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "Science    20397\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    body = soup.body\n",
    "    return body.get_text(separator=\" \") if body else \"\"\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_html)\n",
    "df.head()\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "is_gpu_enabled = spacy.require_gpu()\n",
    "print(f\"Is GPU enabled: {is_gpu_enabled}\")\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Process the text through the spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    # Return the lemmatized text\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-English samples removed: 474\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply the language detection function\n",
    "df['is_english'] = df['clean_content'].apply(is_english)\n",
    "\n",
    "# Calculate the number of non-English samples\n",
    "non_english_count = df['is_english'].value_counts().get(False, 0)\n",
    "print(f\"Number of non-English samples removed: {non_english_count}\")\n",
    "\n",
    "# Filter out non-English samples\n",
    "df = df[df['is_english']].drop(columns=['is_english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;HTML&gt;\\n&lt;HEAD&gt;\\n&lt;TITLE&gt;Gary Posner's Home Page...</td>\n",
       "      <td>Science</td>\n",
       "      <td>gary posner tampa chapter skeptical odysseys s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt;\\n...</td>\n",
       "      <td>Science</td>\n",
       "      <td>desmog desmog energy science denial justice tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en-US\"&gt;&lt;head itemsc...</td>\n",
       "      <td>Science</td>\n",
       "      <td>primary primary sidebar milsoft utility engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\r\\n\\r\\n&lt;!DOCTYPE html&gt;\\r\\n&lt;html lang=\"en\"&gt;\\r\\...</td>\n",
       "      <td>Science</td>\n",
       "      <td>icon australia science agency climate diets pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html xmlns=\"http://www.w3....</td>\n",
       "      <td>Science</td>\n",
       "      <td>sitemap technology downloads browse cameras sd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content category  \\\n",
       "0  <HTML>\\n<HEAD>\\n<TITLE>Gary Posner's Home Page...  Science   \n",
       "1  <!doctype html>\\n<html lang=\"en-US\">\\n<head>\\n...  Science   \n",
       "2  <!DOCTYPE html><html lang=\"en-US\"><head itemsc...  Science   \n",
       "3  \\r\\n\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"en\">\\r\\...  Science   \n",
       "4  <!DOCTYPE html>\\n\\n<html xmlns=\"http://www.w3....  Science   \n",
       "\n",
       "                                       clean_content  \n",
       "0  gary posner tampa chapter skeptical odysseys s...  \n",
       "1  desmog desmog energy science denial justice tr...  \n",
       "2  primary primary sidebar milsoft utility engine...  \n",
       "3  icon australia science agency climate diets pr...  \n",
       "4  sitemap technology downloads browse cameras sd...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "with open(\"stopwords-en.txt\", \"r\") as file:\n",
    "    stopwords_list = file.read().splitlines()\n",
    "\n",
    "stopwords = set(stopwords_list)\n",
    "custom_stopwords = set([\n",
    "    'contact', 'service', 'policy', 'site', 'privacy', 'support', 'email', 'blog',\n",
    "    'post', 'learn', 'read', 'offer', 'provide', 'include', 'click', 'update',\n",
    "    'feature', 'link', 'search', 'website', 'program', 'start', 'view', 'resource',\n",
    "    'experience', 'list', 'free', 'info', 'shop', 'video', 'share', 'member',\n",
    "    'add', 'start', 'work', 'order', 'day', 'people', 'history', 'office',\n",
    "    'time', 'year', 'event', 'national', 'state', 'high', 'month', 'week', 'open',\n",
    "    'cookies', 'menu', 'cart', 'browser', 'select', 'choose', 'hope', 'enjoy', 'disabled',\n",
    "    'facebook', 'twitter', 'youtube', 'instagram', 'account', 'cookie', 'subscribe',\n",
    "    'newsletter', 'sign', 'message', 'comment', 'form', 'login', 'user', 'member',\n",
    "    'join', 'write', 'update', 'search', 'review',\n",
    "    'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n",
    "    'september', 'october', 'november', 'december', 'year', 'today', 'yesterday', 'tomorrow', 'datum', 'date',\n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec',\n",
    "    'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun',\n",
    "    'regional', 'albuquerque', 'chicago', 'minneapolis', 'philadelphia', 'phoenix', 'rhode', 'island', 'scottsdale', 'washington', 'wisconsin', 'michigan',\n",
    "    'bay', 'beach', 'dakota', 'florida', 'georgia', 'hampshire', 'harbor', 'iowa', 'maine',  'missouri', 'park', 'virginia', 'vista', 'wisconsin', 'massachusetts',\n",
    "    'minnesota',\n",
    "    'skip', 'content', 'main', 'term', 'condition', 'toggle', 'navigation', 'wordpress', 'social', 'medium', 'upcoming', 'event',\n",
    "    'photo', 'gallery', 'news', 'frequently', 'question', 'ask', 'press', 'release', 'quick', 'link', 'continue', 'read', 'phone', 'fax', 'answer', 'question',\n",
    "    'board', 'director', 'real', 'estate', 'los', 'angeles', 'new', 'york', 'city', 'san', 'francisco', 'power', 'united', 'kingdom', 'states', 'america', 'fran', 'ais',\n",
    "    'north', 'carolina', 'las', 'vegas', 'annual', 'report', 'highly', 'recommend', 'rss', 'feed', 'white', 'paper', 'hong', 'kong', 'credit', 'card', 'mental', 'health', 'public', 'save', 'money',\n",
    "    'annual', 'meeting', 'wide', 'range', 'care', 'gift', 'professional', 'live', 'stream', 'quality', 'product', 'project', 'management', 'meet', 'nonprofit', 'organization', 'blogthis', 'pinter',\n",
    "    'design', 'success', 'story', 'summer', 'camp', 'chain', 'register', 'trademark', 'username', 'password', 'certificate', 'plan', 'visit', 'regular', 'price', 'covid', 'pandemic', 'south', 'africa', 'west', 'east', 'regional',\n",
    "\n",
    "    'services', 'university', 'software', 'online', 'student', 'products', 'company', 'copyright', 'membership', 'international', 'field', 'book', 'center', 'conference', 'application',\n",
    "    'industry', 'business', 'team', 'base', 'follow', 'download', 'school', 'customer', 'current', 'award', 'close', 'issue', 'access', 'late', 'lead', 'address', 'result', 'series', 'build',\n",
    "    'explore', 'future', 'digital', 'source', 'solutions', 'standard', 'level', 'version', 'type', 'publish',\n",
    "    'society', 'events', 'community', 'training', 'education', 'equipment', 'change', 'map', 'create', 'control', 'links', 'systems', 'image', 'publications', 'article', 'language',\n",
    "    'develop', 'department', 'tool', 'association', 'staff', 'client', 'guide', 'library', 'request', 'mission', 'special', 'network', 'set', 'require', 'complete', 'local', 'overview',\n",
    "    'family', 'virtual', 'american', 'survey', 'advanced', 'media', 'cost', 'receive', 'market', 'graduate', 'improve', 'programs', 'faculty', 'awards', 'focus', 'opportunity', 'career',\n",
    "    'apply',\n",
    "    'solution', 'process', 'reserve', 'check', 'donate', 'archive', 'easy', 'publication', 'sample', 'activity', 'english', 'custom', 'knowledge', 'location', 'job', 'collection', 'country',\n",
    "    'protect', 'learning', 'careers', 'planning', 'committee', 'expert', 'practice', 'light', 'government', 'studies', 'class', 'partner', 'building', 'understand', 'performance',\n",
    "    'supply', 'challenge', 'monitoring', 'pdf', 'structure', 'key', 'economic', 'risk', 'treatment', 'increase', 'oil', 'zoo',\n",
    "    'faq', 'calendar', 'store', 'send', 'projects', 'hold', 'submit', 'usa', 'code', 'enable', 'individual', 'canada', 'workshop', 'connect', 'sale', 'college', 'art', 'foundation',\n",
    "    'relate', 'topic', 'note', 'grow', 'produce', 'facility', 'drive', 'leave', 'culture', 'action', 'cover', 'property', 'assessment', 'approach', 'region', 'word', 'security', 'single',\n",
    "    'flow', 'surface', 'kit',\n",
    "    'rights', 'log', 'forum', 'file', 'statement', 'hour', 'fast', 'bring', 'discover', 'instrument', 'california', 'involve', 'deliver', 'centre', 'students', 'ensure', 'benefit',\n",
    "    'academic', 'major', 'size', 'device', 'undergraduate', 'trust',  'response', 'reduce', 'scale', 'clean', 'dream',\n",
    "    'mail', 'personal', 'database', 'consulting', 'serve', 'talk', 'house', 'author', 'president', 'basic', 'applications', 'commercial',\n",
    "    'idea', 'operation', 'protection', 'professor', 'construction', 'accessories', 'star', 'specific', 'hand', 'complex', 'step', 'generation', 'iso',\n",
    "    'articles', 'videos', 'courses', 'unique', 'reference', 'simple', 'variety', 'fund', 'option', 'short', 'electronic',\n",
    "    'innovation', 'european', 'unit', 'law', 'discovery', 'farm', 'volume', 'rate', 'seminar', 'storage', 'pressure',\n",
    "])\n",
    "stopwords.update(custom_stopwords)\n",
    "stopwords = sorted(stopwords)\n",
    "\n",
    "# Function to further clean the text\n",
    "def further_clean_text(text, stopwords):\n",
    "    # Normalize spaces; replaces all kinds of whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove all numbers (digits) from the text\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Convert text to lower case to standardize for stopwords removal\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split text into words, remove short words and stopwords\n",
    "    text = ' '.join([word for word in text.split() if len(word) >= 3 and word not in stopwords])\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(lambda x: further_clean_text(x, stopwords))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science', 'resources', 'development', 'technology', 'analysis', 'water', 'data', 'study', 'life', 'environmental', 'engineering', 'journal', 'energy', 'scientific', 'testing', 'model', 'material', 'laboratory', 'technical', 'safety', 'institute', 'space', 'human', 'land', 'global', 'lab', 'plant', 'air', 'cell', 'environment', 'conservation', 'food', 'industrial', 'animal', 'method', 'sciences', 'natural', 'impact', 'gas', 'earth', 'registration', 'phd', 'box', 'production', 'physics', 'medical', 'theory', 'manufacturing', 'astronomy', 'volunteer', 'climate', 'chemical', 'function', 'scientist', 'nature', 'biology', 'measurement', 'watch', 'engineer', 'math', 'platform', 'processing', 'marine', 'weather', 'chemistry', 'record', 'australia', 'leadership', 'researcher', 'solar', 'effective', 'county', 'grant', 'specie', 'child', 'category', 'webinar', 'temperature', 'clinical', 'green', 'telescope', 'measure', 'sustainable', 'imaging', 'position', 'wildlife', 'launch', 'waste', 'protein', 'disease', 'dna', 'ocean', 'expand', 'force', 'soil', 'filter', 'molecular', 'organic', 'carbon', 'gene']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_content'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_keywords = [feature_names[i] for i in mean_tfidf.argsort()[::-1]]\n",
    "\n",
    "# Print as a Python array\n",
    "print(top_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyCaret)",
   "language": "python",
   "name": "pycaret_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
