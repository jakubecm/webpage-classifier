{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\r\\n\\r\\n&lt;!doctype html&gt;\\r\\n&lt;html id=\"ctl00_htm...</td>\n",
       "      <td>Reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r\\n\\r\\n&lt;!doctype html&gt;\\r\\n&lt;html id=\"ctl00_htm...</td>\n",
       "      <td>Reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1...</td>\n",
       "      <td>Reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"no-js\" lang=\"en\"...</td>\n",
       "      <td>Reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n&lt;!DOCTYPE html&gt;\\r\\n&lt;!--[if IE ...</td>\n",
       "      <td>Reference</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content   category\n",
       "0  \\r\\n\\r\\n<!doctype html>\\r\\n<html id=\"ctl00_htm...  Reference\n",
       "1  \\r\\n\\r\\n<!doctype html>\\r\\n<html id=\"ctl00_htm...  Reference\n",
       "2  \\r\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1...  Reference\n",
       "3  <!DOCTYPE html>\\n<html class=\"no-js\" lang=\"en\"...  Reference\n",
       "4  \\r\\n\\r\\n\\r\\n\\r\\n<!DOCTYPE html>\\r\\n<!--[if IE ...  Reference"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Establish connection using SQLAlchemy\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password@localhost:5432/dataset_bakalarka')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT content, category\n",
    "FROM (\n",
    "  SELECT content, category,\n",
    "         ROW_NUMBER() OVER (PARTITION BY category ORDER BY RANDOM()) AS rn\n",
    "  FROM web_data\n",
    "  WHERE category = 'Reference'\n",
    ") sub\n",
    "WHERE rn <= 18602\n",
    "\"\"\"\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_sql_query(query, engine, chunksize=10000):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_19288\\2064381985.py:4: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(text, \"lxml\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "Reference    18602\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    body = soup.body\n",
    "    return body.get_text(separator=\" \") if body else \"\"\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_html)\n",
    "df.head()\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "is_gpu_enabled = spacy.require_gpu()\n",
    "print(f\"Is GPU enabled: {is_gpu_enabled}\")\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Process the text through the spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    # Return the lemmatized text\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "df['lemmatized_content'] = df['clean_content'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-English samples removed: 500\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply the language detection function\n",
    "df['is_english'] = df['lemmatized_content'].apply(is_english)\n",
    "\n",
    "# Calculate the number of non-English samples\n",
    "non_english_count = df['is_english'].value_counts().get(False, 0)\n",
    "print(f\"Number of non-English samples removed: {non_english_count}\")\n",
    "\n",
    "# Filter out non-English samples\n",
    "df = df[df['is_english']].drop(columns=['is_english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_content</th>\n",
       "      <th>lemmatized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\r\\n\\r\\n&lt;!doctype html&gt;\\r\\n&lt;html id=\"ctl00_htm...</td>\n",
       "      <td>Reference</td>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Skip...</td>\n",
       "      <td>northern university official northern universi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r\\n\\r\\n&lt;!doctype html&gt;\\r\\n&lt;html id=\"ctl00_htm...</td>\n",
       "      <td>Reference</td>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Skip...</td>\n",
       "      <td>university official unh wildcats schedule sche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1...</td>\n",
       "      <td>Reference</td>\n",
       "      <td>\\n \\n \\n \\n \\n All The Jargon You  Need To Be ...</td>\n",
       "      <td>jargon happy random jargon sea lawyer educate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"no-js\" lang=\"en\"...</td>\n",
       "      <td>Reference</td>\n",
       "      <td>\\n \\n Skip Header and Navigation \\n \\n \\n \\n \\...</td>\n",
       "      <td>header university nebraska lincoln profile pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n&lt;!DOCTYPE html&gt;\\r\\n&lt;!--[if IE ...</td>\n",
       "      <td>Reference</td>\n",
       "      <td>\\n \\n \\n \\n Christendom Crusaders \\n Christend...</td>\n",
       "      <td>christendom crusaders christendom crusaders co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content   category  \\\n",
       "0  \\r\\n\\r\\n<!doctype html>\\r\\n<html id=\"ctl00_htm...  Reference   \n",
       "1  \\r\\n\\r\\n<!doctype html>\\r\\n<html id=\"ctl00_htm...  Reference   \n",
       "2  \\r\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1...  Reference   \n",
       "3  <!DOCTYPE html>\\n<html class=\"no-js\" lang=\"en\"...  Reference   \n",
       "4  \\r\\n\\r\\n\\r\\n\\r\\n<!DOCTYPE html>\\r\\n<!--[if IE ...  Reference   \n",
       "\n",
       "                                       clean_content  \\\n",
       "0  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Skip...   \n",
       "1  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Skip...   \n",
       "2  \\n \\n \\n \\n \\n All The Jargon You  Need To Be ...   \n",
       "3  \\n \\n Skip Header and Navigation \\n \\n \\n \\n \\...   \n",
       "4  \\n \\n \\n \\n Christendom Crusaders \\n Christend...   \n",
       "\n",
       "                                  lemmatized_content  \n",
       "0  northern university official northern universi...  \n",
       "1  university official unh wildcats schedule sche...  \n",
       "2  jargon happy random jargon sea lawyer educate ...  \n",
       "3  header university nebraska lincoln profile pro...  \n",
       "4  christendom crusaders christendom crusaders co...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "with open(\"stopwords-en.txt\", \"r\") as file:\n",
    "    stopwords_list = file.read().splitlines()\n",
    "\n",
    "stopwords = set(stopwords_list)\n",
    "custom_stopwords = set([\n",
    "    'contact', 'service', 'policy', 'site', 'privacy', 'support', 'email', 'blog',\n",
    "    'post', 'learn', 'read', 'offer', 'provide', 'include', 'click', 'update',\n",
    "    'feature', 'link', 'search', 'website', 'program', 'start', 'view', 'resource',\n",
    "    'experience', 'list', 'free', 'info', 'shop', 'video', 'share', 'member',\n",
    "    'add', 'start', 'work', 'order', 'day', 'people', 'history', 'office',\n",
    "    'time', 'year', 'event', 'national', 'state', 'high', 'month', 'week', 'open',\n",
    "    'cookies', 'menu', 'cart', 'browser', 'select', 'choose', 'hope', 'enjoy', 'disabled',\n",
    "    'facebook', 'twitter', 'youtube', 'instagram', 'account', 'cookie', 'subscribe',\n",
    "    'newsletter', 'sign', 'message', 'comment', 'form', 'login', 'user', 'member',\n",
    "    'join', 'write', 'update', 'search', 'review',\n",
    "    'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n",
    "    'september', 'october', 'november', 'december', 'year', 'today', 'yesterday', 'tomorrow', 'datum', 'date',\n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec',\n",
    "    'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun',\n",
    "    'regional', 'albuquerque', 'chicago', 'minneapolis', 'philadelphia', 'phoenix', 'rhode', 'island', 'scottsdale', 'washington', 'wisconsin', 'michigan',\n",
    "    'bay', 'beach', 'dakota', 'florida', 'georgia', 'hampshire', 'harbor', 'iowa', 'maine',  'missouri', 'park', 'virginia', 'vista', 'wisconsin', 'massachusetts',\n",
    "    'minnesota',\n",
    "    'skip', 'content', 'main', 'term', 'condition', 'toggle', 'navigation', 'wordpress', 'social', 'medium', 'upcoming', 'event',\n",
    "    'photo', 'gallery', 'news', 'frequently', 'question', 'ask', 'press', 'release', 'quick', 'link', 'continue', 'read', 'phone', 'fax', 'answer', 'question',\n",
    "    'board', 'director', 'real', 'estate', 'los', 'angeles', 'new', 'york', 'city', 'san', 'francisco', 'power', 'united', 'kingdom', 'states', 'america', 'fran', 'ais',\n",
    "    'north', 'carolina', 'las', 'vegas', 'annual', 'report', 'highly', 'recommend', 'rss', 'feed', 'white', 'paper', 'hong', 'kong', 'credit', 'card', 'mental', 'health', 'public', 'save', 'money',\n",
    "    'annual', 'meeting', 'wide', 'range', 'care', 'gift', 'professional', 'live', 'stream', 'quality', 'product', 'project', 'management', 'meet', 'nonprofit', 'organization', 'blogthis', 'pinter',\n",
    "    'design', 'success', 'story', 'summer', 'camp', 'chain', 'register', 'trademark', 'username', 'password', 'certificate', 'plan', 'visit', 'regular', 'price', 'covid', 'pandemic', 'south', 'africa', 'west', 'east', 'regional',\n",
    "\n",
    "    # Stopwords that hold no contextual meaning for reference\n",
    "    'copyright', 'online', 'society', 'business', 'membership', 'company', 'events', 'check', 'financial', 'car', 'hour', 'posts', 'leave', 'close', 'reviews', 'type', 'feel', 'special',\n",
    "    'late', 'travel', 'local', 'base', 'pay', 'popular', 'lot', 'insurance', 'cost', 'market', 'zoom', 'american', 'receive', 'club', 'game', 'cover', 'debt', 'job', 'process', 'option', 'mattress',\n",
    "    'school', 'museum', 'community', 'life', 'family', 'child', 'field', 'award', 'team', 'follow', 'create', 'tour', 'change', 'country', 'safety', 'law', 'conference', 'athletics', 'basketball',\n",
    "    'track', 'box', 'sports', 'women', 'cross', 'soccer', 'golf', 'baseball', 'tennis', 'volleyball', 'final', 'softball', 'score', 'recap',\n",
    "    'donate', 'career', 'explore', 'current', 'development', 'training', 'opportunity', 'links', 'leadership', 'media', 'future', 'association', 'connect', 'foundation',\n",
    "    'president', 'music', 'hall', 'statement', 'fall', 'spring', 'human', 'awards', 'employment', 'level', 'result', 'major', 'skill', 'diversity', 'football', 'administration',\n",
    "    'volunteer', 'house', 'parent', 'activity', 'texas', 'build', 'series', 'building', 'schools', 'develop', 'discover', 'adult', 'lead', 'culture', 'data', 'impact', 'industry',\n",
    "    'practice', 'opportunities', 'network', 'environment', 'knowledge', 'title', 'policies', 'lacrosse', 'swimming', 'stay', 'watch', 'committee', 'facility',\n",
    "    'serve', 'play', 'location', 'space', 'issue', 'directions', 'fund', 'personal', 'complete', 'focus', 'careers', 'senior', 'security', 'stories', 'innovation', 'communication',\n",
    "    'person', 'challenge', 'california', 'win', 'visitor', 'store', 'food', 'executive', 'prepare', 'council', 'marketing', 'athletic',\n",
    "    'apply', 'international', 'roster', 'archive', 'set',  'associate', 'hockey', 'love', 'notice', 'vision', 'pre', 'fees', 'essay', 'art', 'business', 'health', 'sports', 'sport',\n",
    "    'reserve', 'street', 'age', 'require', 'individual', 'engagement', 'finance', 'county', 'emergency', 'code', 'nursing', 'faq', 'center', 'staff', 'services', 'calendar', 'virtual',\n",
    "    'admissions', 'arts', 'access', 'request', 'aid', 'directory', 'language', 'guide', 'overview', 'address', 'digital', 'centre', 'accessibility', 'english', 'software', 'county',\n",
    "    'medical', 'article', 'transfer', 'session', 'medicine', 'lab', 'funding', 'excellence', 'planning', 'code', 'environmental', 'emergency', 'tech', 'nursing',\n",
    "    'rights', 'host', 'games', 'intranet', 'webinar', 'gift', 'government', 'leader', 'john', 'legal', 'parents', 'military', 'creative', 'jobs', 'partner', 'submit', 'talk', 'portal',\n",
    "    'bring', 'improve', 'celebrate', 'central', 'step', 'cambridge', 'goal', 'justice', 'diving', 'assistant', 'ticket', 'drive', 'corporate', 'previous', 'division', 'head', 'earn', 'window',\n",
    "    'download', 'strategic', 'affairs', 'spanish', 'affairs', 'advanced', 'forms', 'inclusion', 'return', 'christian', 'material', 'tool', 'involve', 'hold', 'action', 'fee', 'unique', 'private', 'water', 'cancel',\n",
    "    'friend', 'memorial', 'subject', 'outreach', 'performance', 'linkedin', 'linkedin', 'coronavirus', 'linkedin', 'counseling',\n",
    "    'hours', 'word', 'grow', 'woman', 'equity', 'athlete',\n",
    "    \n",
    "])\n",
    "stopwords.update(custom_stopwords)\n",
    "stopwords = sorted(stopwords)\n",
    "\n",
    "# Function to further clean the text\n",
    "def further_clean_text(text, stopwords):\n",
    "    # Normalize spaces; replaces all kinds of whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove all numbers (digits) from the text\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Convert text to lower case to standardize for stopwords removal\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split text into words, remove short words and stopwords\n",
    "    text = ' '.join([word for word in text.split() if len(word) >= 3 and word not in stopwords])\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['lemmatized_content'] = df['lemmatized_content'].apply(lambda x: further_clean_text(x, stopwords))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student', 'university', 'college', 'campus', 'education', 'schedule', 'faculty', 'programs', 'resources', 'library', 'graduate', 'students', 'study', 'learning', 'science', 'academic', 'alumni', 'book', 'class', 'collection', 'department', 'technology', 'undergraduate', 'admission', 'mission', 'studies', 'engineering', 'map', 'institute', 'degree', 'courses', 'teacher', 'exhibit', 'application', 'sciences', 'educational', 'global', 'academics', 'teaching', 'scholarship', 'alumnus', 'collections', 'master', 'registration', 'curriculum', 'hours', 'tuition', 'exhibition', 'professor', 'publications', 'scholarships', 'phd', 'image', 'facilities', 'word', 'teach', 'cultural', 'workshop', 'activities', 'archives', 'grant', 'classroom', 'programme', 'grow', 'catalog', 'academy', 'grade', 'maps', 'honor', 'classes', 'woman', 'institution', 'graduation', 'libraries', 'technical', 'journal', 'degrees', 'colleges', 'housing', 'accreditation', 'exam', 'commencement', 'prospective', 'equity', 'semester', 'athlete', 'postgraduate', 'dean', 'departments', 'math', 'mba', 'ncaa', 'biology', 'requirements', 'bachelor', 'physics', 'honors', 'diploma', 'minor', 'psychology']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['lemmatized_content'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_keywords = [feature_names[i] for i in mean_tfidf.argsort()[::-1]]\n",
    "\n",
    "# Print as a Python array\n",
    "print(top_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyCaret)",
   "language": "python",
   "name": "pycaret_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
